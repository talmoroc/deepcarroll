{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from util import *\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "class SonnetModel(object):\n",
    "    #class où on def le modèle de langage\n",
    "    \n",
    "    \n",
    "    def get_last_hidden(self, h, xlen):\n",
    "\n",
    "        ids = tf.range(tf.shape(xlen)[0])\n",
    "        gather_ids = tf.concat([tf.expand_dims(ids, 1), tf.expand_dims(xlen-1, 1)], 1)\n",
    "        return tf.gather_nd(h, gather_ids)\n",
    "\n",
    "\n",
    "    #La porte qui selectionne\n",
    "    def gated_layer(self, s, h, sdim, hdim):\n",
    "\n",
    "        update_w = tf.compat.v1.get_variable(\"update_w\", [sdim+hdim, hdim])\n",
    "        update_b = tf.compat.v1.get_variable(\"update_b\", [hdim], initializer=tf.constant_initializer(1.0))\n",
    "        reset_w  = tf.compat.v1.get_variable(\"reset_w\", [sdim+hdim, hdim])\n",
    "        reset_b  = tf.compat.v1.get_variable(\"reset_b\", [hdim], initializer=tf.constant_initializer(1.0))\n",
    "        c_w      = tf.compat.v1.get_variable(\"c_w\", [sdim+hdim, hdim])\n",
    "        c_b      = tf.compat.v1.get_variable(\"c_b\", [hdim], initializer=tf.constant_initializer())\n",
    "\n",
    "        z = tf.sigmoid(tf.matmul(tf.concat([s, h], 1), update_w) + update_b)\n",
    "        r = tf.sigmoid(tf.matmul(tf.concat([s, h], 1), reset_w) + reset_b)\n",
    "        c = tf.tanh(tf.matmul(tf.concat([s, r*h], 1), c_w) + c_b)\n",
    "        \n",
    "        return (1-z)*h + z*c\n",
    "\n",
    "    #Pareil qu'avant\n",
    "    def selective_encoding(self, h, s, hdim):\n",
    "\n",
    "        h1 = tf.shape(h)[0]\n",
    "        h2 = tf.shape(h)[1]\n",
    "        h_ = tf.reshape(h, [-1, hdim])\n",
    "        s_ = tf.reshape(tf.tile(s, [1, h2]), [-1, hdim])\n",
    "\n",
    "        attend_w = tf.compat.v1.get_variable(\"attend_w\", [hdim*2, hdim])\n",
    "        attend_b = tf.compat.v1.get_variable(\"attend_b\", [hdim], initializer=tf.constant_initializer())\n",
    "\n",
    "        g = tf.sigmoid(tf.matmul(tf.concat([h_, s_], 1), attend_w) + attend_b)\n",
    "\n",
    "        return tf.reshape(h_* g, [h1, h2, -1])\n",
    "\n",
    "    \n",
    "    #Initialisation de la classe\n",
    "    def __init__(self, is_training, batch_size, word_type_size, char_type_size, space_id, pad_id, cf):\n",
    "\n",
    "        #pour la config\n",
    "        self.config = cf\n",
    "\n",
    "\n",
    "        ##############\n",
    "        #placeholders#\n",
    "        ##############\n",
    "\n",
    "        #language model placeholders\n",
    "        self.lm_x    = tf.compat.v1.placeholder(tf.int32, [None, None])\n",
    "        self.lm_xlen = tf.compat.v1.placeholder(tf.int32, [None])\n",
    "        self.lm_y    = tf.compat.v1.placeholder(tf.int32, [None, None])\n",
    "        self.lm_hist = tf.compat.v1.placeholder(tf.int32, [None, None])\n",
    "        self.lm_hlen = tf.compat.v1.placeholder(tf.int32, [None])\n",
    "\n",
    "\n",
    "        ################\n",
    "        #language model#\n",
    "        ################\n",
    "        with tf.compat.v1.variable_scope(\"language_model\"):\n",
    "            self.init_lm(is_training, batch_size, word_type_size)\n",
    "\n",
    "    \n",
    "    \n",
    "    ################\n",
    "    #Language model#\n",
    "    ################\n",
    "    \n",
    "    \n",
    "    # -- language model network\n",
    "    def init_lm(self, is_training, batch_size, word_type_size):\n",
    "        \n",
    "        #On mets la config de base\n",
    "        cf = self.config\n",
    "        \n",
    "        \n",
    "        #shared word embeddings (used by encoder and decoder)\n",
    "        self.word_embedding = tf.compat.v1.get_variable(\"word_embedding\", [word_type_size, cf.word_embedding_dim],\n",
    "            initializer=tf.random_uniform_initializer(-0.05/cf.word_embedding_dim, 0.05/cf.word_embedding_dim))\n",
    "    \n",
    "        #########\n",
    "        #decoder#\n",
    "        #########\n",
    "\n",
    "        #define lstm cells\n",
    "        lm_dec_cell = tf.compat.v1.nn.rnn_cell.LSTMCell(cf.lm_dec_dim, use_peepholes=True, forget_bias=1.0)\n",
    "        if is_training and cf.keep_prob < 1.0:\n",
    "            lm_dec_cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(lm_dec_cell, output_keep_prob=cf.keep_prob)\n",
    "        self.lm_dec_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([lm_dec_cell] * cf.lm_dec_layer_size)\n",
    "\n",
    "        #initial states\n",
    "        self.lm_initial_state = self.lm_dec_cell.zero_state(batch_size, tf.float32)\n",
    "        state = self.lm_initial_state\n",
    "\n",
    "        #pad symbol vocab ID = 0; create mask = 1.0 where vocab ID > 0 else 0.0\n",
    "        lm_mask = tf.cast(tf.greater(self.lm_x, tf.zeros(tf.shape(self.lm_x), dtype=tf.int32)), dtype=tf.float32)\n",
    "\n",
    "        #embedding lookup\n",
    "        word_inputs = tf.nn.embedding_lookup(self.word_embedding, self.lm_x)\n",
    "        if is_training and cf.keep_prob < 1.0:\n",
    "            word_inputs = tf.nn.dropout(word_inputs, cf.keep_prob)\n",
    "\n",
    "        #process character encodings\n",
    "        #concat last hidden state of fw RNN with first hidden state of bw RNN\n",
    "        #fw_hidden = self.get_last_hidden(self.char_encodings[0], self.pm_enc_xlen)\n",
    "        #char_inputs = tf.concat(1, [fw_hidden, self.char_encodings[1][:,0,:]])\n",
    "        #char_inputs = tf.reshape(char_inputs, [batch_size, -1, cf.pm_enc_dim*2]) #reshape into same dimension as inputs\n",
    "        \n",
    "        #concat word and char encodings\n",
    "        #inputs = tf.concat(2, [word_inputs, char_inputs])\n",
    "        inputs = word_inputs\n",
    "\n",
    "        #apply mask to zero out pad embeddings\n",
    "        inputs = inputs * tf.expand_dims(lm_mask, -1)\n",
    "\n",
    "        #dynamic rnn\n",
    "        dec_outputs, final_state = tf.nn.dynamic_rnn(self.lm_dec_cell, inputs, sequence_length=self.lm_xlen, \\\n",
    "            dtype=tf.float32, initial_state=self.lm_initial_state)\n",
    "        self.lm_final_state = final_state\n",
    "\n",
    "        #########################\n",
    "        #encoder (history words)#\n",
    "        #########################\n",
    "\n",
    "        #embedding lookup\n",
    "        hist_inputs = tf.nn.embedding_lookup(self.word_embedding, self.lm_hist)\n",
    "        if is_training and cf.keep_prob < 1.0:\n",
    "            hist_inputs = tf.nn.dropout(hist_inputs, cf.keep_prob)\n",
    "\n",
    "        #encoder lstm cell\n",
    "        lm_enc_cell = tf.nn.rnn_cell.LSTMCell(cf.lm_enc_dim, forget_bias=1.0)\n",
    "        if is_training and cf.keep_prob < 1.0:\n",
    "            lm_enc_cell = tf.nn.rnn_cell.DropoutWrapper(lm_enc_cell, output_keep_prob=cf.keep_prob)\n",
    "\n",
    "        #history word encodings\n",
    "        hist_outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=lm_enc_cell, cell_bw=lm_enc_cell,\n",
    "            inputs=hist_inputs, sequence_length=self.lm_hlen, dtype=tf.float32)\n",
    "\n",
    "        #full history encoding\n",
    "        full_encoding = tf.concat( [hist_outputs[0][:,-1,:], hist_outputs[1][:,0,:]], axis=1)\n",
    "\n",
    "        #concat fw and bw hidden states\n",
    "        hist_outputs = tf.concat(hist_outputs, axis=2)\n",
    "\n",
    "        #selective encoding\n",
    "        with tf.variable_scope(\"selective_encoding\"):\n",
    "            hist_outputs = self.selective_encoding(hist_outputs, full_encoding, cf.lm_enc_dim*2)\n",
    "\n",
    "        #attention (concat)\n",
    "        with tf.variable_scope(\"lm_attention\"):\n",
    "            attend_w = tf.get_variable(\"attend_w\", [cf.lm_enc_dim*2+cf.lm_dec_dim, cf.lm_attend_dim])\n",
    "            attend_b = tf.get_variable(\"attend_b\", [cf.lm_attend_dim], initializer=tf.constant_initializer())\n",
    "            attend_v = tf.get_variable(\"attend_v\", [cf.lm_attend_dim, 1])\n",
    "\n",
    "        enc_steps = tf.shape(hist_outputs)[1]\n",
    "        dec_steps = tf.shape(dec_outputs)[1]\n",
    "\n",
    "        #prepare encoder and decoder\n",
    "        hist_outputs_t = tf.tile(hist_outputs, [1, dec_steps, 1])\n",
    "        dec_outputs_t  = tf.reshape(tf.tile(dec_outputs, [1, 1, enc_steps]),\n",
    "            [batch_size, -1, cf.lm_dec_dim])\n",
    "\n",
    "        #compute e\n",
    "        hist_dec_concat = tf.concat( [tf.reshape(hist_outputs_t, [-1, cf.lm_enc_dim*2]),\n",
    "            tf.reshape(dec_outputs_t, [-1, cf.lm_dec_dim])], 1)\n",
    "        e = tf.matmul(tf.tanh(tf.matmul(hist_dec_concat, attend_w) + attend_b), attend_v)\n",
    "        e = tf.reshape(e, [-1, enc_steps])\n",
    "\n",
    "        #mask out pad symbols to compute alpha and weighted sum of history words\n",
    "        alpha    = tf.reshape(tf.nn.softmax(e), [batch_size, -1, 1])\n",
    "        context  = tf.reduce_sum(tf.reshape(alpha * hist_outputs_t,\n",
    "            [batch_size,dec_steps,enc_steps,-1]), 2)\n",
    "\n",
    "        #save attention weights\n",
    "        self.lm_attentions = tf.reshape(alpha, [batch_size, dec_steps, enc_steps])\n",
    "\n",
    "        ##############\n",
    "        #output layer#\n",
    "        ##############\n",
    "\n",
    "        #reshape both into [batch_size*len, hidden_dim]\n",
    "        dec_outputs = tf.reshape(dec_outputs, [-1, cf.lm_dec_dim])\n",
    "        context     = tf.reshape(context, [-1, cf.lm_enc_dim*2])\n",
    "        \n",
    "        #combine context and decoder hidden state with a gated unit\n",
    "        with tf.variable_scope(\"gated_unit\"):\n",
    "            hidden = self.gated_layer(context, dec_outputs, cf.lm_enc_dim*2, cf.lm_dec_dim)\n",
    "            #hidden = dec_outputs\n",
    "\n",
    "        #output embeddings\n",
    "        lm_output_proj = tf.get_variable(\"lm_output_proj\", [cf.word_embedding_dim, cf.lm_dec_dim])\n",
    "        lm_softmax_b   = tf.get_variable(\"lm_softmax_b\", [word_type_size], initializer=tf.constant_initializer())\n",
    "        lm_softmax_w   = tf.transpose(tf.tanh(tf.matmul(self.word_embedding, lm_output_proj)))\n",
    "\n",
    "        #compute logits and cost\n",
    "        lm_logits     = tf.matmul(hidden, lm_softmax_w) + lm_softmax_b\n",
    "        lm_crossent   = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=lm_logits, labels=tf.reshape(self.lm_y, [-1]))\n",
    "        lm_crossent_m = lm_crossent * tf.reshape(lm_mask, [-1])\n",
    "        self.lm_cost  = tf.reduce_sum(lm_crossent_m) / batch_size\n",
    "\n",
    "        if not is_training:\n",
    "            self.lm_probs = tf.nn.softmax(lm_logits)\n",
    "            return\n",
    "\n",
    "        #run optimiser and backpropagate (clipped) gradients for lm loss\n",
    "        lm_tvars = tf.trainable_variables()\n",
    "        lm_grads, _ = tf.clip_by_global_norm(tf.gradients(self.lm_cost, lm_tvars), cf.max_grad_norm)\n",
    "        self.lm_train_op = tf.train.AdagradOptimizer(cf.lm_learning_rate).apply_gradients(zip(lm_grads, lm_tvars))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # -- sample a word given probability distribution (with option to normalise the distribution with temperature)\n",
    "    # -- temperature = 0 means argmax\n",
    "    def sample_word(self, sess, probs, temperature, unk_symbol_id, pad_symbol_id, idxword, avoid_words):\n",
    "\n",
    "\n",
    "        if temperature == 0:\n",
    "            return np.argmax(probs)\n",
    "\n",
    "        probs = probs.astype(np.float64) #convert to float64 for higher precision\n",
    "        probs = np.log(probs) / temperature\n",
    "        probs = np.exp(probs) / math.fsum(np.exp(probs))\n",
    "\n",
    "        #avoid unk_symbol_id if possible\n",
    "        sampled = None\n",
    "        \n",
    "        for i in range(1000):\n",
    "            sampled = np.argmax(np.random.multinomial(1, probs, 1))\n",
    "\n",
    "            #resample if it's a word to avoid\n",
    "            if sampled in avoid_words:\n",
    "                continue\n",
    "\n",
    "            return sampled\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    # -- generate a sentence by sampling one word at a time\n",
    "    def sample_sent(self, sess, state, x, hist, hlen, avoid_symbols, stopwords,temp_min, temp_max,\n",
    "        unk_symbol_id, pad_symbol_id, end_symbol_id, space_id, idxword, last_words, max_words):\n",
    "\n",
    "        def filter_stop_symbol(word_ids):\n",
    "            cleaned = set([])\n",
    "            for w in word_ids:\n",
    "                if w not in (stopwords | set([pad_symbol_id, end_symbol_id])) and not only_symbol(idxword[w]):\n",
    "                    cleaned.add(w)\n",
    "            return cleaned\n",
    "\n",
    "        def get_freq_words(word_ids, freq_threshold):\n",
    "            words     = []\n",
    "            word_freq = Counter(word_ids)\n",
    "            for k, v in word_freq.items():\n",
    "                #if v >= freq_threshold and not only_symbol(idxword[k]) and k != end_symbol_id:\n",
    "                if v >= freq_threshold and k != end_symbol_id:\n",
    "                    words.append(k)\n",
    "            return set(words)\n",
    "\n",
    "        sent   = []\n",
    "\n",
    "        while True:\n",
    "            probs, state = sess.run([self.lm_probs, self.lm_final_state],\n",
    "                {self.lm_x: x, self.lm_initial_state: state, self.lm_xlen: [1],\n",
    "                self.lm_hist: hist, self.lm_hlen: hlen})\n",
    "\n",
    "            #avoid words previously generated\n",
    "            avoid_words = filter_stop_symbol(sent + hist[0])\n",
    "            freq_words  = get_freq_words(sent + hist[0], 2) #avoid any words that occur >= N times\n",
    "            avoid_words = avoid_words | freq_words | set(sent[-3:] + last_words + avoid_symbols + [unk_symbol_id])\n",
    "            \n",
    "            \n",
    "            #self, sess, probs, temperature, unk_symbol_id, pad_symbol_id, wordxchar, idxword, avoid_words\n",
    "            word = self.sample_word(sess, probs[0], np.random.uniform(temp_min, temp_max), unk_symbol_id,\n",
    "                pad_symbol_id, idxword, avoid_words)\n",
    "\n",
    "            if word != None:\n",
    "                sent.append(word)\n",
    "                x             = [[ sent[-1] ]]\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                return None, None\n",
    "                \n",
    "            if sent[-1] == end_symbol_id or len(sent) >= max_words:\n",
    "\n",
    "                if len(sent) > 1:\n",
    "                    return sent, state\n",
    "                else:\n",
    "                    return None, None\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # -- quatrain generation\n",
    "    def generate(self, sess, idxword, pad_symbol_id, end_symbol_id, unk_symbol_id, space_id,\n",
    "        avoid_symbols, stopwords, temp_min, temp_max, max_lines, max_words, sent_sample, verbose=False):\n",
    "\n",
    "        def reset():\n",
    "            state       = sess.run(self.lm_dec_cell.zero_state(1, tf.float32))\n",
    "            prev_state  = state\n",
    "            x           = [[end_symbol_id]]\n",
    "            sonnet      = []\n",
    "            sent_probs  = []\n",
    "            last_words  = []\n",
    "            total_words = 0\n",
    "            total_lines = 0\n",
    "            \n",
    "            return state, prev_state, x, sonnet, sent_probs, last_words, total_words, total_lines\n",
    "\n",
    "        end_symbol = idxword[end_symbol_id]\n",
    "        sent_temp  = 0.1 #sentence sampling temperature\n",
    "\n",
    "        state, prev_state, x, sonnet, sent_probs, last_words, total_words, total_lines = reset()\n",
    "\n",
    "        #verbose prints during generation\n",
    "        if verbose:\n",
    "            sys.stdout.write(\"  Number of generated lines = 0/4\\r\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        while total_words < max_words and total_lines < max_lines:\n",
    "\n",
    "            #add history context\n",
    "            if len(sonnet) == 0 or sonnet.count(end_symbol_id) < 1:\n",
    "                hist = [[unk_symbol_id] + [pad_symbol_id]*5]\n",
    "            else:\n",
    "                hist = [sonnet + [pad_symbol_id]*5]\n",
    "            hlen = [len(hist[0])]\n",
    "\n",
    "            \n",
    "\n",
    "            #genereate N sentences and sample from them (using softmax(-one_pl) as probability)\n",
    "            compteur = 0\n",
    "            all_sent, all_state = [], []\n",
    "            for _ in range(sent_sample):\n",
    "            \n",
    "           \n",
    "                one_sent, one_state = self.sample_sent(sess, state, x, hist, hlen, avoid_symbols, stopwords, temp_min, temp_max, unk_symbol_id, pad_symbol_id, end_symbol_id, space_id, idxword, last_words, max_words)\n",
    "                    \n",
    "                if one_sent != None:\n",
    "                    all_sent.append(one_sent)\n",
    "                    all_state.append(one_state)\n",
    "                    compteur += 1\n",
    "                    \n",
    "                else:\n",
    "                    all_sent = []\n",
    "                    break\n",
    "\n",
    "            #unable to generate sentences; reset whole quatrain\n",
    "            if len(all_sent) == 0:\n",
    "\n",
    "                state, prev_state, x, sonnet, sent_probs, last_words, total_words, total_lines = reset()\n",
    "\n",
    "            else:\n",
    "                \n",
    "                probs = [1/compteur] * compteur\n",
    "\n",
    "                #sample a sentence\n",
    "                sent_id = np.argmax(np.random.multinomial(1, probs, 1))\n",
    "                sent    = all_sent[sent_id]\n",
    "                state   = all_state[sent_id]\n",
    "\n",
    "                total_words += len(sent)\n",
    "                total_lines += 1\n",
    "                prev_state   = state\n",
    "\n",
    "                sonnet.extend(sent)\n",
    "                last_words.append(sent[0])\n",
    "\n",
    "            if verbose:\n",
    "                sys.stdout.write(\"  Number of generated lines = %d/4\\r\" % (total_lines))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        #postprocessing\n",
    "        sonnet = sonnet[:-1] if sonnet[-1] == end_symbol_id else sonnet\n",
    "        sonnet = [ postprocess_sentence(item) for item in \\\n",
    "            \" \".join(list(reversed([ idxword[item] for item in sonnet ]))).strip().split(end_symbol) ]\n",
    "\n",
    "        return sonnet, list(reversed(sent_probs))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
