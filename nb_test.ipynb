{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config_etienne as cf\n",
    "import sys\n",
    "import codecs\n",
    "import random\n",
    "import time\n",
    "import imp\n",
    "import os\n",
    "import tensorflow as tf\n",
    "# tf.disable_eager_execution()\n",
    "# tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import gensim.models as g\n",
    "\n",
    "from sonnet_model import SonnetModel\n",
    "from RUN_epoch import run_epoch\n",
    "\n",
    "from util import *\n",
    "\n",
    "#constants\n",
    "pad_symbol = \"<pad>\"\n",
    "end_symbol = \"<eos>\"\n",
    "unk_symbol = \"<unk>\"\n",
    "dummy_symbols = [pad_symbol, end_symbol, unk_symbol]\n",
    "\n",
    "#globals\n",
    "wordxid = None\n",
    "idxword = None\n",
    "charxid = None\n",
    "idxchar = None\n",
    "wordxchar = None #word id to [char ids]\n",
    "rhyme_thresholds = [0.9, 0.8, 0.7, 0.6]\n",
    "stress_acc_threshold = 0.4\n",
    "reset_scale = 1.05\n",
    "\n",
    "\n",
    "sys.stdout = codecs.getwriter('utf-8')(sys.stdout)\n",
    "\n",
    "#set the seeds\n",
    "random.seed(cf.seed)\n",
    "np.random.seed(cf.seed)\n",
    "\n",
    "#load word embedding model if given and set word embedding size\n",
    "if cf.word_embedding_model:\n",
    "    #print(\"\\nLoading word embedding model...\")\n",
    "    mword = g.Word2Vec.load(cf.word_embedding_model)\n",
    "    cf.word_embedding_dim= mword.vector_size\n",
    "\n",
    "#load vocab\n",
    "print(\"\\n\", \"First pass to collect word and character vocabulary...\")\n",
    "idxword, wordxid, idxchar, charxid, wordxchar = load_vocab(cf.train_data, cf.word_minfreq, dummy_symbols)\n",
    "print(\"\\nWord type size =\", len(idxword))\n",
    "print(\"\\nChar type size =\", len(idxchar))\n",
    "\n",
    "#load train and valid data\n",
    "print(\"\\n Loading train and valid data...\")\n",
    "train_word_data, train_char_data, train_nwords, train_nchars, train_rhyme_data = \\\n",
    "    load_data(cf.train_data, wordxid, idxword, charxid, idxchar, dummy_symbols)\n",
    "# a rajouter\n",
    "\n",
    "\n",
    "valid_word_data, valid_char_data, valid_rhyme_data, valid_nwords, valid_nchars = \\\n",
    "    load_data(cf.valid_data, wordxid, idxword, charxid, idxchar, dummy_symbols)\n",
    "print_stats(\"\\nTrain\", train_word_data, train_nwords, train_nchars, train_rhyme_data)\n",
    "print_stats(\"\\nValid\", valid_word_data, valid_rhyme_data, valid_nwords, valid_nchars)\n",
    "\n",
    "#load test data if it's given\n",
    "if cf.test_data:\n",
    "    test_word_data, test_char_data, test_rhyme_data, test_nwords, test_nchars = \\\n",
    "        load_data(cf.test_data, wordxid, idxword, charxid, idxchar, dummy_symbols)\n",
    "    print_stats(\"\\nTest\", test_word_data, test_rhyme_data, test_nwords, test_nchars)\n",
    "\n",
    "if cf.word_embedding_model:\n",
    "            word_emb = init_embedding(mword, idxword)\n",
    "\n",
    "word_emb_dict = dict(zip(idxword, word_emb))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def closest_word(word):\n",
    "    \n",
    "    dico = {}\n",
    "    \n",
    "    def cosinus_similarity(word1, word2):\n",
    "        score =  np.dot(word_emb_dict[word1], word_emb_dict[word2])/(np.linalg.norm(word_emb_dict [word1])* \\\n",
    "                                                                         np.linalg.norm(word_emb_dict [word2]))\n",
    "        return {word2 : score}\n",
    "    \n",
    "    \n",
    "    for ele in word_emb_dict.keys():\n",
    "        dico.update(cosinus_similarity(word, ele))\n",
    "    \n",
    "    sorted_dico = sorted(dico.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        \n",
    "    return OrderedDict(sorted_dico)\n",
    "\n",
    "\n",
    "create_word_batch(data=train_word_data, batch_size=32, lines_per_doc=14, nlines_per_batch=2, pad_symbol=pad_symbol,\\\n",
    "                  end_symbol=end_symbol, unk_symbol=unk_symbol, shuffle_data=True)[0]\n",
    "\n",
    "\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        tf.set_random_seed(cf.seed)\n",
    "        with tf.variable_scope(\"model\", reuse=None):\n",
    "            mtrain = SonnetModel(True, cf.batch_size, len(idxword), len(idxchar),\n",
    "                charxid[\" \"], charxid[pad_symbol], cf)\n",
    "        with tf.variable_scope(\"model\", reuse=True):\n",
    "            mvalid = SonnetModel(False, cf.batch_size, len(idxword), len(idxchar),\n",
    "                charxid[\" \"], charxid[pad_symbol], cf)\n",
    "        with tf.variable_scope(\"model\", reuse=True):\n",
    "            mgen = SonnetModel(False, 1, len(idxword), len(idxchar), charxid[\" \"], charxid[pad_symbol], cf)\n",
    "\n",
    "        tf.compat.v1.global_variables_initializer().run()\n",
    "\n",
    "        #initialise word embedding\n",
    "        if cf.word_embedding_model:\n",
    "            word_emb = init_embedding(mword, idxword)\n",
    "            sess.run(mtrain.word_embedding.assign(word_emb))\n",
    "\n",
    "\n",
    "        if cf.save_model:\n",
    "            if not os.path.exists(cf.output_dir):\n",
    "                os.makedirs(cf.output_dir)\n",
    "            #create saver object to save model\n",
    "            saver = tf.compat.v1.train.Saver(max_to_keep=0)\n",
    "\n",
    "\n",
    "        #train model\n",
    "        prev_lm_loss = None \n",
    "\n",
    "        for i in range(cf.epoch_size):\n",
    "\n",
    "            print(\"\\nEpoch =\", i+1)\n",
    "\n",
    "            #create batches for language model\n",
    "            train_word_batch = create_word_batch(train_word_data, cf.batch_size,\n",
    "                cf.doc_lines, cf.bptt_truncate, wordxid[pad_symbol], wordxid[end_symbol], wordxid[unk_symbol], True)\n",
    "\n",
    "            valid_word_batch = create_word_batch(valid_word_data, cf.batch_size,\n",
    "                cf.doc_lines, cf.bptt_truncate, wordxid[pad_symbol], wordxid[end_symbol], wordxid[unk_symbol], False)\n",
    "\n",
    "            #train an epoch\n",
    "            _ = run_epoch(sess, train_word_batch, mtrain, \"TRAIN\", True)\n",
    "            lm_loss = run_epoch(sess, valid_word_batch, mvalid, \"VALID\", False)\n",
    "\n",
    "            #create batch for test model and run an epoch if it's given\n",
    "            if cf.test_data:\n",
    "                test_word_batch = create_word_batch(test_word_data, cf.batch_size,\n",
    "                    cf.doc_lines, cf.bptt_truncate, wordxid[pad_symbol], wordxid[end_symbol], wordxid[unk_symbol], False)\n",
    "                run_epoch(sess, test_word_batch, mvalid, \"TEST\", False)\n",
    "\n",
    "\n",
    "            #We save\n",
    "            if cf.save_model:\n",
    "                    if prev_lm_loss == None  or lm_loss <= prev_lm_loss : #or not train_lm :\n",
    "                        saver.save(sess, os.path.join(cf.output_dir, \"model.ckpt\"))\n",
    "                        prev_lm_loss = lm_loss\n",
    "\n",
    "                    else:\n",
    "                        saver.restore(sess, os.path.join(cf.output_dir, \"model.ckpt\"))\n",
    "                        print(\"New valid performance is worse; restoring previous parameters...\")\n",
    "                        print(\"  lm loss: %.5f --> %.5f\" % (prev_lm_loss, lm_loss))\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "############### GENERATION ##################\n",
    "#############################################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import codecs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple\n",
    "from sonnet_model import SonnetModel\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from util import *\n",
    "\n",
    "#constants\n",
    "seed =  2\n",
    "num_samples = 1\n",
    "#save_pickle = os.path(output_dir)\n",
    "temp_min = 0.6\n",
    "temp_max = 0.8\n",
    "sent_sample = 10\n",
    "verbose = False\n",
    "pad_symbol = \"<pad>\"\n",
    "end_symbol = \"<eos>\"\n",
    "unk_symbol = \"<unk>\"\n",
    "dummy_symbols = [pad_symbol, end_symbol, unk_symbol]\n",
    "custom_stopwords = [ \"thee\", \"thou\", \"thy\", \"'d\", \"'s\", \"'ll\", \"must\", \"shall\" ]\n",
    "\n",
    "###########\n",
    "#functions#\n",
    "###########\n",
    "\n",
    "def reverse_dic(idxvocab):\n",
    "    vocabxid = {}\n",
    "    for vi, v in enumerate(idxvocab):\n",
    "        vocabxid[v] = vi\n",
    "\n",
    "    return vocabxid\n",
    "\n",
    "######\n",
    "#main#\n",
    "######\n",
    "\n",
    "def main():\n",
    "\n",
    "    sys.stdout = codecs.getwriter('utf-8')(sys.stdout)\n",
    "\n",
    "    #set the seeds\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "    #symbols to avoid for generation\n",
    "    avoid_symbols = [\"(\", \")\", \"“\", \"‘\", \"”\", \"’\", \"[\", \"]\"]\n",
    "    avoid_symbols = [ wordxid[item] for item in avoid_symbols ]\n",
    "    stopwords = set([ wordxid[item] for item in (nltk_stopwords.words(\"english\") + custom_stopwords) if item in wordxid ])\n",
    "\n",
    "    quatrains = []\n",
    "    #initialise and load model parameters\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        with tf.variable_scope(\"model\", reuse=None):\n",
    "            mtest = SonnetModel(False, cf.batch_size, len(idxword), len(idxchar), charxid[\" \"], charxid[pad_symbol], cf)\n",
    "\n",
    "        with tf.variable_scope(\"model\", reuse=True):\n",
    "            mgen = SonnetModel(False, 1, len(idxword), len(idxchar), charxid[\" \"], charxid[pad_symbol], cf)\n",
    "\n",
    "        #load tensorflow model\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, os.path.join(cf.output_dir, \"model.ckpt\"))\n",
    "\n",
    "        #quatrain generation\n",
    "        for _ in range(num_samples):\n",
    "\n",
    "            #generate some random sentences\n",
    "            #print(\"\\nTemperature =\", temp_min, \"-\", temp_max)\n",
    "\n",
    "            q, probs = mgen.generate(sess, idxword, wordxid[pad_symbol],\n",
    "                wordxid[end_symbol], wordxid[unk_symbol], charxid[\" \"], avoid_symbols, stopwords,\n",
    "                temp_min, temp_max, 12, 400, sent_sample, verbose)\n",
    "\n",
    "            quatrains.append(q)\n",
    "        return quatrains\n",
    "\n",
    "main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
